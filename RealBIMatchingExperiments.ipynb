{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import DPTFeatureExtractor, DPTForDepthEstimation\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depth Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = DPTFeatureExtractor.from_pretrained(\"Intel/dpt-large\")\n",
    "model = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-large\")\n",
    "\n",
    "\n",
    "def estimate_depth_dpt(image):\n",
    "        # prepare image for the model\n",
    "        encoding = feature_extractor(image, return_tensors=\"pt\")\n",
    "        # forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoding)\n",
    "            predicted_depth = outputs.predicted_depth\n",
    "        # interpolate to original size\n",
    "        prediction = torch.nn.functional.interpolate(\n",
    "                            predicted_depth.unsqueeze(1),\n",
    "                            size=image.size[::-1],\n",
    "                            mode=\"bicubic\",\n",
    "                            align_corners=False,\n",
    "                        ).squeeze()\n",
    "        output = prediction.cpu().numpy()\n",
    "        formatted = (output * 255 / np.max(output)).astype('uint8')\n",
    "        img = Image.fromarray(formatted)\n",
    "        #convert to cv2\n",
    "        img = np.array(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loftr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LoFTR.src.loftr import LoFTR, default_cfg\n",
    "\n",
    "matcher = LoFTR(config=default_cfg)\n",
    "matcher.load_state_dict(torch.load('LoFTR/weights/outdoor_ds.ckpt')['state_dict'])\n",
    "def compute_matches_loftr(fname_depthA,fname_depthB,match_th=0.1):\n",
    "    loftr_matcher = matcher.eval().cuda()\n",
    "    \n",
    "    img0 = torch.from_numpy(fname_depthA)[None][None].cuda() / 255.\n",
    "    img1 = torch.from_numpy(fname_depthB)[None][None].cuda() / 255.\n",
    "    batch = {'image0': img0, 'image1': img1}\n",
    "\n",
    "    # Inference with LoFTR and get prediction\n",
    "    with torch.no_grad():\n",
    "        loftr_matcher(batch)\n",
    "        mkpts0 = batch['mkpts0_f'].cpu().numpy()\n",
    "        mkpts1 = batch['mkpts1_f'].cpu().numpy()\n",
    "        mconf = batch['mconf'].cpu().numpy()\n",
    "    \n",
    "    mkpts0 = mkpts0[mconf>match_th]\n",
    "    mkpts1 = mkpts1[mconf>match_th]\n",
    "    mconf = mconf[mconf>match_th]\n",
    "    return mkpts0, mkpts1, mconf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real vs BIM Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def real_bim_matching(real_img_file,bim_img_file):\n",
    "    real_img = Image.open(real_img_file).convert(\"RGB\").resize((640,480))\n",
    "    bim_img = Image.open(bim_img_file).convert(\"RGB\").resize((640,480))\n",
    "    real_img_depth = estimate_depth_dpt(real_img)\n",
    "    bim_img_depth = estimate_depth_dpt(bim_img)\n",
    "    mkpts0, mkpts1, mconf = compute_matches_loftr(real_img_depth,bim_img_depth)\n",
    "    return np.mean(mconf), len(mconf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Image matching metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "folder_path_real_imgs ='data/Sam/wetransfer_02_images_xmpfiles_2023-11-07_1050/02_Images_XMPfiles'\n",
    "folder_path_bim_imgs = 'data/Sam/wetransfer_02_images_xmpfiles_2023-11-07_1050/04_renderedImages'\n",
    "\n",
    "df = pd.read_csv('ImageNames_List.csv')\n",
    "df_results = pd.DataFrame(columns=['real_img','bim_img','n_matches','avg_score','mutual_information'])\n",
    "\n",
    "print('Computing Matching Scores')\n",
    "for i in tqdm(range(len(df))):\n",
    "    real_img_file = os.path.join(folder_path_real_imgs,df.iloc[i][0]+'.jpeg')\n",
    "    bim_img_file = os.path.join(folder_path_bim_imgs,df.iloc[i][1]+'.png')\n",
    "    real_img_name = real_img_file.split('/')[-1]\n",
    "    bim_img_name = bim_img_file.split('/')[-1]\n",
    "    mean_matching_score, n_matches = real_bim_matching(real_img_file,bim_img_file)\n",
    "    df_results = df_results.append({'real_img':real_img_name, 'bim_img':bim_img_name,'n_matches':n_matches,'avg_score':mean_matching_score},ignore_index=True)\n",
    "# save the results into a csv file\n",
    "df_results.to_csv('MatchingResults.csv',index=False)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
