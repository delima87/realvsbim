{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import DPTFeatureExtractor, DPTForDepthEstimation\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depth Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deep-sewer/.conda/envs/huggingface/lib/python3.10/site-packages/transformers/models/dpt/feature_extraction_dpt.py:28: FutureWarning: The class DPTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use DPTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Some weights of DPTForDepthEstimation were not initialized from the model checkpoint at Intel/dpt-large and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = DPTFeatureExtractor.from_pretrained(\"Intel/dpt-large\")\n",
    "model = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-large\")\n",
    "\n",
    "\n",
    "def estimate_depth_dpt(image):\n",
    "        # prepare image for the model\n",
    "        encoding = feature_extractor(image, return_tensors=\"pt\")\n",
    "        # forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoding)\n",
    "            predicted_depth = outputs.predicted_depth\n",
    "        # interpolate to original size\n",
    "        prediction = torch.nn.functional.interpolate(\n",
    "                            predicted_depth.unsqueeze(1),\n",
    "                            size=image.size[::-1],\n",
    "                            mode=\"bicubic\",\n",
    "                            align_corners=False,\n",
    "                        ).squeeze()\n",
    "        output = prediction.cpu().numpy()\n",
    "        formatted = (output * 255 / np.max(output)).astype('uint8')\n",
    "        img = Image.fromarray(formatted)\n",
    "        #convert to cv2\n",
    "        img = np.array(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loftr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LoFTR.src.loftr import LoFTR, default_cfg\n",
    "\n",
    "matcher = LoFTR(config=default_cfg)\n",
    "def compute_matches_loftr(fname_depthA,fname_depthB,match_th=0.1):\n",
    "    matcher.load_state_dict(torch.load('LoFTR/weights/outdoor_ds.ckpt')['state_dict'])\n",
    "    loftr_matcher = matcher.eval().cuda()\n",
    "    \n",
    "    img0 = torch.from_numpy(fname_depthA)[None][None].cuda() / 255.\n",
    "    img1 = torch.from_numpy(fname_depthB)[None][None].cuda() / 255.\n",
    "    batch = {'image0': img0, 'image1': img1}\n",
    "\n",
    "    # Inference with LoFTR and get prediction\n",
    "    with torch.no_grad():\n",
    "        loftr_matcher(batch)\n",
    "        mkpts0 = batch['mkpts0_f'].cpu().numpy()\n",
    "        mkpts1 = batch['mkpts1_f'].cpu().numpy()\n",
    "        mconf = batch['mconf'].cpu().numpy()\n",
    "    \n",
    "    mkpts0 = mkpts0[mconf>match_th]\n",
    "    mkpts1 = mkpts1[mconf>match_th]\n",
    "    mconf = mconf[mconf>match_th]\n",
    "    return mkpts0, mkpts1, mconf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real vs BIM Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def real_bim_matching(real_img_file,bim_img_file):\n",
    "    real_img = Image.open(real_img_file).convert(\"RGB\")\n",
    "    bim_img = Image.open(bim_img_file).convert(\"RGB\")\n",
    "    real_img_depth = estimate_depth_dpt(real_img)\n",
    "    bim_img_depth = estimate_depth_dpt(bim_img)\n",
    "    mkpts0, mkpts1, mconf = compute_matches_loftr('real_img_depth.png','bim_img_depth.png')\n",
    "    return np.mean(mconf), len(mconf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Image matching metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Matching Scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got str)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/deep-sewer/Workspace/RealvsBim/RealBIMatchingExperiments.ipynb Cell 9\u001b[0m line \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/deep-sewer/Workspace/RealvsBim/RealBIMatchingExperiments.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     real_img_name \u001b[39m=\u001b[39m real_img_file\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/deep-sewer/Workspace/RealvsBim/RealBIMatchingExperiments.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     bim_img_name \u001b[39m=\u001b[39m bim_img_file\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/deep-sewer/Workspace/RealvsBim/RealBIMatchingExperiments.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     mean_matching_score, n_matches \u001b[39m=\u001b[39m real_bim_matching(real_img_file,bim_img_file)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/deep-sewer/Workspace/RealvsBim/RealBIMatchingExperiments.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     df_results \u001b[39m=\u001b[39m df_results\u001b[39m.\u001b[39mappend({\u001b[39m'\u001b[39m\u001b[39mreal_img\u001b[39m\u001b[39m'\u001b[39m:real_img_name, \u001b[39m'\u001b[39m\u001b[39mbim_img\u001b[39m\u001b[39m'\u001b[39m:bim_img_name,\u001b[39m'\u001b[39m\u001b[39mn_matches\u001b[39m\u001b[39m'\u001b[39m:n_matches,\u001b[39m'\u001b[39m\u001b[39mavg_score\u001b[39m\u001b[39m'\u001b[39m:mean_matching_score},ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/deep-sewer/Workspace/RealvsBim/RealBIMatchingExperiments.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# save the results into a csv file\u001b[39;00m\n",
      "\u001b[1;32m/home/deep-sewer/Workspace/RealvsBim/RealBIMatchingExperiments.ipynb Cell 9\u001b[0m line \u001b[0;36mreal_bim_matching\u001b[0;34m(real_img_file, bim_img_file)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/deep-sewer/Workspace/RealvsBim/RealBIMatchingExperiments.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m cv2\u001b[39m.\u001b[39mimwrite(\u001b[39m'\u001b[39m\u001b[39mreal_img_depth.png\u001b[39m\u001b[39m'\u001b[39m,real_img_depth)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/deep-sewer/Workspace/RealvsBim/RealBIMatchingExperiments.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m cv2\u001b[39m.\u001b[39mimwrite(\u001b[39m'\u001b[39m\u001b[39mbim_img_depth.png\u001b[39m\u001b[39m'\u001b[39m,bim_img_depth)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/deep-sewer/Workspace/RealvsBim/RealBIMatchingExperiments.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m mkpts0, mkpts1, mconf \u001b[39m=\u001b[39m compute_matches_loftr(\u001b[39m'\u001b[39;49m\u001b[39mreal_img_depth.png\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mbim_img_depth.png\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/deep-sewer/Workspace/RealvsBim/RealBIMatchingExperiments.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(mconf), \u001b[39mlen\u001b[39m(mconf)\n",
      "\u001b[1;32m/home/deep-sewer/Workspace/RealvsBim/RealBIMatchingExperiments.ipynb Cell 9\u001b[0m line \u001b[0;36mcompute_matches_loftr\u001b[0;34m(fname_depthA, fname_depthB, match_th)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/deep-sewer/Workspace/RealvsBim/RealBIMatchingExperiments.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m matcher\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mLoFTR/weights/outdoor_ds.ckpt\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39mstate_dict\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/deep-sewer/Workspace/RealvsBim/RealBIMatchingExperiments.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m loftr_matcher \u001b[39m=\u001b[39m matcher\u001b[39m.\u001b[39meval()\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/deep-sewer/Workspace/RealvsBim/RealBIMatchingExperiments.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m img0 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfrom_numpy(fname_depthA)[\u001b[39mNone\u001b[39;00m][\u001b[39mNone\u001b[39;00m]\u001b[39m.\u001b[39mcuda() \u001b[39m/\u001b[39m \u001b[39m255.\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/deep-sewer/Workspace/RealvsBim/RealBIMatchingExperiments.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m img1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(fname_depthB)[\u001b[39mNone\u001b[39;00m][\u001b[39mNone\u001b[39;00m]\u001b[39m.\u001b[39mcuda() \u001b[39m/\u001b[39m \u001b[39m255.\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/deep-sewer/Workspace/RealvsBim/RealBIMatchingExperiments.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m batch \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mimage0\u001b[39m\u001b[39m'\u001b[39m: img0, \u001b[39m'\u001b[39m\u001b[39mimage1\u001b[39m\u001b[39m'\u001b[39m: img1}\n",
      "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got str)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "folder_path_real_imgs ='data/Sam/wetransfer_02_images_xmpfiles_2023-11-07_1050/02_Images_XMPfiles'\n",
    "folder_path_bim_imgs = 'data/Sam/wetransfer_02_images_xmpfiles_2023-11-07_1050/04_renderedImages'\n",
    "\n",
    "df = pd.read_csv('ImageNames_List.csv')\n",
    "df_results = pd.DataFrame(columns=['real_img','bim_img','n_matches','avg_score','mutual_information'])\n",
    "\n",
    "print('Computing Matching Scores')\n",
    "for i in tqdm(range(len(df))):\n",
    "    real_img_file = os.path.join(folder_path_real_imgs,df.iloc[i][0]+'.jpeg')\n",
    "    bim_img_file = os.path.join(folder_path_bim_imgs,df.iloc[i][1]+'.png')\n",
    "    real_img_name = real_img_file.split('/')[-1]\n",
    "    bim_img_name = bim_img_file.split('/')[-1]\n",
    "    mean_matching_score, n_matches = real_bim_matching(real_img_file,bim_img_file)\n",
    "    df_results = df_results.append({'real_img':real_img_name, 'bim_img':bim_img_name,'n_matches':n_matches,'avg_score':mean_matching_score},ignore_index=True)\n",
    "# save the results into a csv file\n",
    "df_results.to_csv('MatchingResults.csv',index=False)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
